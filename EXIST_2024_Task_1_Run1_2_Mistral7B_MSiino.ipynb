{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/EXIST2024/blob/main/EXIST_2024_Task_1_Run1_2_Mistral7B_MSiino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In the Run 1 the starting and the ending tag \"s\" within the prompt is used for any few shot sample. In the Run 2 the \"s\" tag is used only once."
      ],
      "metadata": {
        "id": "_vuGzN7UVGL0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ubSCVmmlBKX"
      },
      "source": [
        "Installing dependencies. You might need to tweak the CMAKE_ARGS for the `llama-cpp-python` pip package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKL68Itp9Bm-",
        "outputId": "c56a07a6-de08-4edc-e88a-3f1fd3fc42b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python>=0.1.79\n",
            "  Downloading llama_cpp_python-0.2.71.tar.gz (48.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 MB\u001b[0m \u001b[31m206.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m320.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m234.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m329.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python>=0.1.79)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.71-cp310-cp310-linux_x86_64.whl size=55332522 sha256=15b2e8a86ee262e2d37ec3c6536840a87ce93f17251b39c5bd215f1cfa7dd5b6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zeokkhh8/wheels/5d/35/d3/b4ab42b4528cc5e1f9b1469c4b4076de51e2f3ceb50506b424\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.3\n",
            "    Uninstalling Jinja2-3.1.3:\n",
            "      Successfully uninstalled Jinja2-3.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.71 numpy-1.26.4 typing-extensions-4.11.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.2.2)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ],
      "source": [
        "# GPU llama-cpp-python; Starting from version llama-cpp-python==0.1.79, it supports GGUF\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on \" pip install 'llama-cpp-python>=0.1.79' --force-reinstall --upgrade --no-cache-dir\n",
        "# For download the models\n",
        "!pip install huggingface_hub\n",
        "!pip install datasets\n",
        "!pip install -U deep-translator\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from deep_translator import GoogleTranslator\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "# Seed to shuffle the json training set.\n",
        "seed_value = 42\n",
        "random.seed(seed_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2ns57iDlBKa"
      },
      "source": [
        "Downloading an instruction-finetuned Mistral model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uDMqQmBfAhYO",
        "outputId": "698051f5-af54-490a-e268-10e8ced580ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b681ea880f56420eacb3fc0366dc65b6",
            "da972865bee547b6998fcc38e88165dc",
            "5b19be7f8ae645c6a264d794091d6940",
            "2f04869e637542018dee1995a3fca139",
            "858936563e4442b0a0bc168a9b30d3dc",
            "7de4cf1cfff847c39b30876b4ef2d1e4",
            "800e8c46c9ae4a118168b9e0ed9bd453",
            "ea0a151eec96406281bfcfa3a70245bf",
            "cec3fa17191843d089bf40785a6de01b",
            "a7dbe5f4d8614b1992e8a1bf66783ef3",
            "0e0454888c0647f4a0ecf54b19488707"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mistral-7b-instruct-v0.2.Q6_K.gguf:   0%|          | 0.00/5.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b681ea880f56420eacb3fc0366dc65b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q6_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q6_K:  226 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q6_K\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 32/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  5461.00 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 8192\n",
            "llama_new_context_with_model: n_batch    = 800\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 4\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
            "Guessed chat format: mistral-instruct\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\"\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "# This config has been tested on an RTX 3080 (VRAM of 16GB).\n",
        "# you might need to tweak with respect to your hardware.\n",
        "from llama_cpp import Llama\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=4, #16, # CPU cores\n",
        "    n_batch=800, #8000, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32, # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=8192, # Context window\n",
        "    logits_all=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset for the three subtasks."
      ],
      "metadata": {
        "id": "jeXgLOpd4ztp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subtask 1.\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/EXIST2024/main/dataset/EXIST2023_test_clean.json'\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/EXIST2024/main/dataset/EXIST2024_training.json'\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/EXIST2024/main/dataset/EXIST2024_training_task1_gold_hard.json'\n",
        "\n",
        "\n",
        "train_path = 'EXIST2024_training.json'\n",
        "train_gold_path = 'EXIST2024_training_task1_gold_hard.json'\n",
        "test_path = 'EXIST2023_test_clean.json'"
      ],
      "metadata": {
        "id": "fKaC_tQ-2xbI",
        "outputId": "5ec4e285-3568-4b6e-eb61-42327b44c6fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-09 13:46:46--  https://raw.githubusercontent.com/marco-siino/EXIST2024/main/dataset/EXIST2023_test_clean.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2143912 (2.0M) [text/plain]\n",
            "Saving to: ‘EXIST2023_test_clean.json’\n",
            "\n",
            "EXIST2023_test_clea 100%[===================>]   2.04M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2024-05-09 13:46:47 (245 MB/s) - ‘EXIST2023_test_clean.json’ saved [2143912/2143912]\n",
            "\n",
            "--2024-05-09 13:46:47--  https://raw.githubusercontent.com/marco-siino/EXIST2024/main/dataset/EXIST2024_training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9397366 (9.0M) [text/plain]\n",
            "Saving to: ‘EXIST2024_training.json’\n",
            "\n",
            "EXIST2024_training. 100%[===================>]   8.96M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-05-09 13:46:48 (319 MB/s) - ‘EXIST2024_training.json’ saved [9397366/9397366]\n",
            "\n",
            "--2024-05-09 13:46:48--  https://raw.githubusercontent.com/marco-siino/EXIST2024/main/dataset/EXIST2024_training_task1_gold_hard.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 433241 (423K) [text/plain]\n",
            "Saving to: ‘EXIST2024_training_task1_gold_hard.json’\n",
            "\n",
            "EXIST2024_training_ 100%[===================>] 423.09K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2024-05-09 13:46:48 (79.0 MB/s) - ‘EXIST2024_training_task1_gold_hard.json’ saved [433241/433241]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create few-shot samples from training set."
      ],
      "metadata": {
        "id": "fHPX3E0PCJ9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " prompt_context = \"[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\\n\"\n"
      ],
      "metadata": {
        "id": "zafRW5vU6y1n"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(train_path, 'r') as istr:\n",
        "    train_1_json = json.load(istr)\n",
        "  #print(train_1_json[0])\n",
        "\n",
        "# Convert the dictionary keys into a list\n",
        "keys_list = list(train_1_json.keys())\n",
        "\n",
        "# Shuffle the list\n",
        "random.shuffle(keys_list)\n",
        "\n",
        "# Create a new dictionary with shuffled keys\n",
        "train_1_json = {key: train_1_json[key] for key in keys_list}"
      ],
      "metadata": {
        "id": "lsmWAy7VoXuP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_few_shot_samples_run1(train_1_json,nr_samples):\n",
        "\n",
        "  nr_few_shot_samples = nr_samples\n",
        "\n",
        "  few_shot_counter = 0\n",
        "\n",
        "  few_shot_samples = ''\n",
        "\n",
        "  with open(train_gold_path, 'r') as istr:\n",
        "    train_gold_json = json.load(istr)\n",
        "  indexed_data = {entry[\"id\"]: entry for entry in train_gold_json}\n",
        "\n",
        "  for key, value in train_1_json.items():\n",
        "\n",
        "    if few_shot_counter == nr_few_shot_samples:\n",
        "      break\n",
        "    try:\n",
        "      label = indexed_data[key]['value']\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "    few_shot_counter+=1\n",
        "    tweet = train_1_json[key]['tweet']\n",
        "    print(train_1_json[key]['tweet'] + ' // ' + label)\n",
        "    if train_1_json[key]['lang']=='es':\n",
        "      tweet = GoogleTranslator(source='es', target='en').translate(train_1_json[key]['tweet'])\n",
        "\n",
        "    few_shot_samples += '<s>'+prompt_context+\"TWEET:\"+tweet + ' [/INST] \\n ' + label + \"\\n</s> \\n\\n \"\n",
        "\n",
        "    if few_shot_counter == nr_few_shot_samples:\n",
        "        break\n",
        "\n",
        "  return few_shot_samples"
      ],
      "metadata": {
        "id": "99ag8gav51mC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_few_shot_samples_run2(train_1_json,nr_samples):\n",
        "\n",
        "  nr_few_shot_samples = nr_samples\n",
        "\n",
        "  few_shot_counter = 0\n",
        "\n",
        "  few_shot_samples = ''\n",
        "\n",
        "  with open(train_gold_path, 'r') as istr:\n",
        "    train_gold_json = json.load(istr)\n",
        "  indexed_data = {entry[\"id\"]: entry for entry in train_gold_json}\n",
        "  #print(indexed_data['100001'])\n",
        "\n",
        "  for key, value in train_1_json.items():\n",
        "\n",
        "    if few_shot_counter == nr_few_shot_samples:\n",
        "      break\n",
        "    try:\n",
        "      label = indexed_data[key]['value']\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "    few_shot_counter+=1\n",
        "    tweet = train_1_json[key]['tweet']\n",
        "    print(train_1_json[key]['tweet'] + ' // ' + label)\n",
        "    if train_1_json[key]['lang']=='es':\n",
        "      tweet = GoogleTranslator(source='es', target='en').translate(train_1_json[key]['tweet'])\n",
        "\n",
        "    few_shot_samples += prompt_context+\"TWEET:\"+tweet + ' [/INST] \\n ' + label + \"\\n \\n\\n \"\n",
        "\n",
        "    if few_shot_counter == nr_few_shot_samples:\n",
        "        break\n",
        "\n",
        "  return few_shot_samples"
      ],
      "metadata": {
        "id": "oNuu3aAhZxYA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_samples_run1 = create_few_shot_samples_run1(train_1_json,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8IaDH-7GmrN",
        "outputId": "bf0507a9-efff-44f6-c781-851d706a6692"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@JackelineWB @JohnMAckerman @lopezobrador_ tu eres el absurdo, o sea Loret si tiene derecho de violar la privacidad de personas civiles?? // NO\n",
            "Yo me pregunto si que haya fortunas genera acaso un techo de cristal o algo por el estilo. Cada vez hay más ricos, y a quien le moleste lo heredado, que analice si padece de envidia. https://t.co/oesBZ03sdh // NO\n",
            "No te vistas así*No uses minifalta*No se saques selfie*No salgas sola* Parecés una puta*Vos lo habrás mirado*Estás provocandoAh ! y que deprimente lo de Fablet. El señor que nos dice qué tenemos que hacer para que no nos violen. // YES\n",
            "@Russtophocles1 Girls don’t count as reply guys , call me sexist but I always love compliments from da gals. Even if they are horny posting😂😂 but if you’re mean I will cry // YES\n",
            "Siempre he pensado esto de todos los abogados ... Jajaja Bueno mi cuñado NO es así, me sorprende porque es el estereotipo perfecto! Costeño, ebrio y abogado https://t.co/YH6AjDlElH // NO\n",
            "Es algo relacionado con la identidad de género y la expresión de género. Me gusta skk seguros de su identidad como chicos pero amando usar vestidos, solo que para llegar a esa parte tienen que pasar por problemas de descubrimiento, y es aquí donde Chuuya ayuda a Dazai 🤍. // NO\n",
            "You want to be a strong and empowered woman? Then grow a spine strong enough to look at your own sins and shortcomings square in the eye and be willing to face them head on. Only weak women blame men for everything. // YES\n",
            "Maldito amor romantico me dañaste sapoperro gonorrea // NO\n",
            "@ArcEng9154644 @pnjaban @MrAndyNgo @alankesslr @Claudio_Report To see these antifa people work so hard to destroy what they use and need would be amusing, but their beliefs and antics cause too much damage to the rest of society for me to consider them just clueless clowns. // NO\n",
            "🌵 El cine y las palomitas: un matrimonio forzado con final feliz... https://t.co/jf50mSjhwI // NO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_samples_run2 = create_few_shot_samples_run2(train_1_json,10)"
      ],
      "metadata": {
        "id": "fdVNwQQhaMlH",
        "outputId": "d59c76cb-8e57-4bec-a783-c5a601eec866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@JackelineWB @JohnMAckerman @lopezobrador_ tu eres el absurdo, o sea Loret si tiene derecho de violar la privacidad de personas civiles?? // NO\n",
            "Yo me pregunto si que haya fortunas genera acaso un techo de cristal o algo por el estilo. Cada vez hay más ricos, y a quien le moleste lo heredado, que analice si padece de envidia. https://t.co/oesBZ03sdh // NO\n",
            "No te vistas así*No uses minifalta*No se saques selfie*No salgas sola* Parecés una puta*Vos lo habrás mirado*Estás provocandoAh ! y que deprimente lo de Fablet. El señor que nos dice qué tenemos que hacer para que no nos violen. // YES\n",
            "@Russtophocles1 Girls don’t count as reply guys , call me sexist but I always love compliments from da gals. Even if they are horny posting😂😂 but if you’re mean I will cry // YES\n",
            "Siempre he pensado esto de todos los abogados ... Jajaja Bueno mi cuñado NO es así, me sorprende porque es el estereotipo perfecto! Costeño, ebrio y abogado https://t.co/YH6AjDlElH // NO\n",
            "Es algo relacionado con la identidad de género y la expresión de género. Me gusta skk seguros de su identidad como chicos pero amando usar vestidos, solo que para llegar a esa parte tienen que pasar por problemas de descubrimiento, y es aquí donde Chuuya ayuda a Dazai 🤍. // NO\n",
            "You want to be a strong and empowered woman? Then grow a spine strong enough to look at your own sins and shortcomings square in the eye and be willing to face them head on. Only weak women blame men for everything. // YES\n",
            "Maldito amor romantico me dañaste sapoperro gonorrea // NO\n",
            "@ArcEng9154644 @pnjaban @MrAndyNgo @alankesslr @Claudio_Report To see these antifa people work so hard to destroy what they use and need would be amusing, but their beliefs and antics cause too much damage to the rest of society for me to consider them just clueless clowns. // NO\n",
            "🌵 El cine y las palomitas: un matrimonio forzado con final feliz... https://t.co/jf50mSjhwI // NO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(few_shot_samples_run1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WHgxqxAxcu1",
        "outputId": "ef9d702a-67ea-4a08-b89f-dc0c46d34f26"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@JackelineWB @JohnMAckerman @lopezobrador_ you are absurd, that is, Loret has the right to violate the privacy of civilians?? [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:I wonder if the fact that there are fortunes generates a glass ceiling or something like that. There are more and more rich people, and anyone who is bothered by what they inherited should analyze whether they suffer from envy. https://t.co/oesBZ03sdh [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:Don't dress like that*Don't wear a mini skirt*Don't take a selfie*Don't go out alone* You look like a whore*You must have seen it*You're provokingAh! and how depressing about Fablet. The man who tells us what we have to do so that they do not rape us. [/INST] \n",
            " YES\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@Russtophocles1 Girls don’t count as reply guys , call me sexist but I always love compliments from da gals. Even if they are horny posting😂😂 but if you’re mean I will cry [/INST] \n",
            " YES\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:I have always thought this about all lawyers... Hahaha Well my brother-in-law is NOT like that, it surprises me because he is the perfect stereotype! Costeño, drunk and lawyer https://t.co/YH6AjDlElH [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:It's something related to gender identity and gender expression. I like skk sure of their identity as boys but loving to wear dresses, only to get to that part they have to go through discovery problems, and this is where Chuuya helps Dazai 🤍. [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:You want to be a strong and empowered woman? Then grow a spine strong enough to look at your own sins and shortcomings square in the eye and be willing to face them head on. Only weak women blame men for everything. [/INST] \n",
            " YES\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:Damn romantic love, you hurt me toadstool gonorrhea [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@ArcEng9154644 @pnjaban @MrAndyNgo @alankesslr @Claudio_Report To see these antifa people work so hard to destroy what they use and need would be amusing, but their beliefs and antics cause too much damage to the rest of society for me to consider them just clueless clowns. [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:🌵 Cinema and popcorn: a forced marriage with a happy ending... https://t.co/jf50mSjhwI [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(few_shot_samples_run2)"
      ],
      "metadata": {
        "id": "HSfwZXX5aXSE",
        "outputId": "ad646b4f-5b68-411a-c34c-0d5016793af9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@JackelineWB @JohnMAckerman @lopezobrador_ you are absurd, that is, Loret has the right to violate the privacy of civilians?? [/INST] \n",
            " NO\n",
            " \n",
            "\n",
            " [INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:I wonder if the fact that there are fortunes generates a glass ceiling or something like that. There are more and more rich people, and anyone who is bothered by what they inherited should analyze whether they suffer from envy. https://t.co/oesBZ03sdh [/INST] \n",
            " NO\n",
            " \n",
            "\n",
            " [INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:Don't dress like that*Don't wear a mini skirt*Don't take a selfie*Don't go out alone* You look like a whore*You must have seen it*You're provokingAh! and how depressing about Fablet. The man who tells us what we have to do so that they do not rape us. [/INST] \n",
            " YES\n",
            " \n",
            "\n",
            " [INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@Russtophocles1 Girls don’t count as reply guys , call me sexist but I always love compliments from da gals. Even if they are horny posting😂😂 but if you’re mean I will cry [/INST] \n",
            " YES\n",
            " \n",
            "\n",
            " [INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:I have always thought this about all lawyers... Hahaha Well my brother-in-law is NOT like that, it surprises me because he is the perfect stereotype! Costeño, drunk and lawyer https://t.co/YH6AjDlElH [/INST] \n",
            " NO\n",
            " \n",
            "\n",
            " [INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:It's something related to gender identity and gender expression. I like skk sure of their identity as boys but loving to wear dresses, only to get to that part they have to go through discovery problems, and this is where Chuuya helps Dazai 🤍. [/INST] \n",
            " NO\n",
            " \n",
            "\n",
            " [INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:You want to be a strong and empowered woman? Then grow a spine strong enough to look at your own sins and shortcomings square in the eye and be willing to face them head on. Only weak women blame men for everything. [/INST] \n",
            " YES\n",
            " \n",
            "\n",
            " [INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:Damn romantic love, you hurt me toadstool gonorrhea [/INST] \n",
            " NO\n",
            " \n",
            "\n",
            " [INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@ArcEng9154644 @pnjaban @MrAndyNgo @alankesslr @Claudio_Report To see these antifa people work so hard to destroy what they use and need would be amusing, but their beliefs and antics cause too much damage to the rest of society for me to consider them just clueless clowns. [/INST] \n",
            " NO\n",
            " \n",
            "\n",
            " [INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:🌵 Cinema and popcorn: a forced marriage with a happy ending... https://t.co/jf50mSjhwI [/INST] \n",
            " NO\n",
            " \n",
            "\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run!"
      ],
      "metadata": {
        "id": "r6Cqo0JqotN9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKo1-X5OvT4b",
        "outputId": "9afc2e95-7d26-4ec6-d8ad-d059faebdd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      11.26 ms /    16 runs   (    0.70 ms per token,  1421.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =     953.35 ms /    31 tokens (   30.75 ms per token,    32.52 tokens per second)\n",
            "llama_print_timings:        eval time =    1825.91 ms /    15 runs   (  121.73 ms per token,     8.22 tokens per second)\n",
            "llama_print_timings:       total time =    3000.86 ms /    46 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@Eurogamer_es All gamergate from development to game forums, classic in the world of video games.\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       8.84 ms /    16 runs   (    0.55 ms per token,  1809.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =     554.04 ms /    32 tokens (   17.31 ms per token,    57.76 tokens per second)\n",
            "llama_print_timings:        eval time =    1479.82 ms /    15 runs   (   98.65 ms per token,    10.14 tokens per second)\n",
            "llama_print_timings:       total time =    2169.46 ms /    47 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@ArCaNgEl__23 @Benzenazi Man, it's not comparable, look at Gamergate.\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.09 ms /    16 runs   (    0.57 ms per token,  1760.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     577.26 ms /    24 tokens (   24.05 ms per token,    41.58 tokens per second)\n",
            "llama_print_timings:        eval time =    1508.51 ms /    15 runs   (  100.57 ms per token,     9.94 tokens per second)\n",
            "llama_print_timings:       total time =    2204.03 ms /    39 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "I'm looking for companies involved in gamergate for etc etc to buy all their products\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      10.42 ms /    16 runs   (    0.65 ms per token,  1535.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =     266.35 ms /    75 tokens (    3.55 ms per token,   281.59 tokens per second)\n",
            "llama_print_timings:        eval time =    1706.46 ms /    15 runs   (  113.76 ms per token,     8.79 tokens per second)\n",
            "llama_print_timings:       total time =    2234.81 ms /    90 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@jordirico First it was the internet, then the gamergate, the manosphere and its extreme misogyny without the platforms lifting a finger, the incel-murders, the extreme right capitalizing on hate, the mimicry of these ideas with those of a respectable debate and finally here we are\n",
            "[/INST]\n",
            "YES\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      11.24 ms /    16 runs   (    0.70 ms per token,  1423.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     859.61 ms /    28 tokens (   30.70 ms per token,    32.57 tokens per second)\n",
            "llama_print_timings:        eval time =    1912.53 ms /    15 runs   (  127.50 ms per token,     7.84 tokens per second)\n",
            "llama_print_timings:       total time =    2988.06 ms /    43 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@AlonsoQuijano12 I was involved in gamergate, so unfortunately I know too much 😭\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       8.94 ms /    16 runs   (    0.56 ms per token,  1788.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     775.03 ms /    29 tokens (   26.73 ms per token,    37.42 tokens per second)\n",
            "llama_print_timings:        eval time =    1532.26 ms /    15 runs   (  102.15 ms per token,     9.79 tokens per second)\n",
            "llama_print_timings:       total time =    2442.70 ms /    44 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@MrSandman1954 @AKN4710 Gamergate screwed the internet forever.\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.32 ms /    16 runs   (    0.58 ms per token,  1716.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =     299.13 ms /   102 tokens (    2.93 ms per token,   340.99 tokens per second)\n",
            "llama_print_timings:        eval time =    1500.46 ms /    15 runs   (  100.03 ms per token,    10.00 tokens per second)\n",
            "llama_print_timings:       total time =    2122.31 ms /   117 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@grupoeldeber @grupoeldeber It would be good if they also said that it was in CANADA🇨🇦 the first country where it was reported that G∆y couples who adopted children abused them in 2008. How selective are they with the crap they publish, right? https://t.co/GPAmLOfcY9 https://t.co/f36aAmxq35\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.13 ms /    16 runs   (    0.57 ms per token,  1752.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =     252.47 ms /    40 tokens (    6.31 ms per token,   158.43 tokens per second)\n",
            "llama_print_timings:        eval time =    1557.09 ms /    15 runs   (  103.81 ms per token,     9.63 tokens per second)\n",
            "llama_print_timings:       total time =    1971.28 ms /    55 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "They, very close together and more alone than ever. The consequences of everything. Mozambique. https://t.co/5nVGTb3Ua3\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       8.99 ms /    16 runs   (    0.56 ms per token,  1779.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =     586.76 ms /    32 tokens (   18.34 ms per token,    54.54 tokens per second)\n",
            "llama_print_timings:        eval time =    1530.86 ms /    15 runs   (  102.06 ms per token,     9.80 tokens per second)\n",
            "llama_print_timings:       total time =    2260.74 ms /    47 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "DC Children's Hospital Harassed Over Trans Youth Serviceshttps://t.co/RrEtZ5TDDM\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.33 ms /    16 runs   (    0.58 ms per token,  1714.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     256.55 ms /    45 tokens (    5.70 ms per token,   175.40 tokens per second)\n",
            "llama_print_timings:        eval time =    1568.85 ms /    15 runs   (  104.59 ms per token,     9.56 tokens per second)\n",
            "llama_print_timings:       total time =    2011.60 ms /    60 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "20mins deep into BeReal and I have never felt so pressured to take my phone nope I don't want to feel harassed I say goodbye 🫡\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      11.79 ms /    16 runs   (    0.74 ms per token,  1357.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     271.85 ms /    61 tokens (    4.46 ms per token,   224.39 tokens per second)\n",
            "llama_print_timings:        eval time =    1789.32 ms /    15 runs   (  119.29 ms per token,     8.38 tokens per second)\n",
            "llama_print_timings:       total time =    2435.20 ms /    76 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "Anthony Fauci, the leading US infectious disease expert, noted in August 2022 that he had received death threats and that his family was routinely exposed to harassment. https://t.co/EerbO2Px5j\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       8.92 ms /    16 runs   (    0.56 ms per token,  1794.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =     699.63 ms /    28 tokens (   24.99 ms per token,    40.02 tokens per second)\n",
            "llama_print_timings:        eval time =    1435.95 ms /    15 runs   (   95.73 ms per token,    10.45 tokens per second)\n",
            "llama_print_timings:       total time =    2263.21 ms /    43 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@dimplerrylover I know but literally the girl was being harassed like how can she smile like that\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.62 ms /    16 runs   (    0.60 ms per token,  1663.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     284.15 ms /    85 tokens (    3.34 ms per token,   299.14 tokens per second)\n",
            "llama_print_timings:        eval time =    1480.67 ms /    15 runs   (   98.71 ms per token,    10.13 tokens per second)\n",
            "llama_print_timings:       total time =    2055.73 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@marcela_ruge @_Lamanada_ @Mitocondria1311 @karinin7986 @julibmoya @MissPoirot07 @Adrianaruiz17 @DiaAnimales @marcevalenciar @MONYRODRIGUEZOF Marce do you have daviplata?\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      11.71 ms /    16 runs   (    0.73 ms per token,  1366.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     260.71 ms /    64 tokens (    4.07 ms per token,   245.48 tokens per second)\n",
            "llama_print_timings:        eval time =    1804.13 ms /    15 runs   (  120.28 ms per token,     8.31 tokens per second)\n",
            "llama_print_timings:       total time =    2311.64 ms /    79 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "About to see the grand premiere of #Jauría based on the case of #LaManada @anasofihg @RobertoBeck90 @elteatrocdmx #teatromexicano https://t.co/N5Uke18ikQ\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.59 ms /    16 runs   (    0.60 ms per token,  1668.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     259.75 ms /    64 tokens (    4.06 ms per token,   246.39 tokens per second)\n",
            "llama_print_timings:        eval time =    1512.29 ms /    15 runs   (  100.82 ms per token,     9.92 tokens per second)\n",
            "llama_print_timings:       total time =    1995.70 ms /    79 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "What a bastard our Uncle Pablo is with his little jokes. How comfortable we are on the cool grass... mybestfriends #doglove #theyneverabandon #lodaneverythingchangedenada https://t.co/4x20oe7V8D\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      10.20 ms /    16 runs   (    0.64 ms per token,  1569.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     289.27 ms /    75 tokens (    3.86 ms per token,   259.28 tokens per second)\n",
            "llama_print_timings:        eval time =    1606.43 ms /    15 runs   (  107.10 ms per token,     9.34 tokens per second)\n",
            "llama_print_timings:       total time =    2152.33 ms /    90 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@noebermudez_13 @ldacr And we are already looking forward to filling the stands with you everywhere again!!! @LaManada_CR And now distribute balls left and right except to the ball gatherers 🥺🤣🤣🤣🤣#VAMOSLEONAS #SOMOSLAMANADA\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      10.94 ms /    16 runs   (    0.68 ms per token,  1462.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =     311.53 ms /   116 tokens (    2.69 ms per token,   372.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1832.49 ms /    15 runs   (  122.17 ms per token,     8.19 tokens per second)\n",
            "llama_print_timings:       total time =    2545.53 ms /   131 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "#TBT: María Rosa, Gregorio Polanco and Ysabel Verás, back in October 2016, during one of the Kermés held for the team's fundraising. The Pack is tradition, family, unity and joy!🥎🐺🥎🐺#TBT #Auuuuu#LaManada#LobosFansClub#LobosSoftbolClub https://t.co/LwSL4npanZ\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.20 ms /    16 runs   (    0.58 ms per token,  1738.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =     382.56 ms /   129 tokens (    2.97 ms per token,   337.21 tokens per second)\n",
            "llama_print_timings:        eval time =    1610.81 ms /    15 runs   (  107.39 ms per token,     9.31 tokens per second)\n",
            "llama_print_timings:       total time =    2401.78 ms /   144 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "AUGUST 9 Find out everything! 🎙 You can watch the entire segment of 'El Bla, Bla, Bla' with HÉCTOR JOAQUÍN in the #Podcasts section of the LaMusica app. 📲#HectorJoaquin #LaManada #ElBlaBlaBla #LaMusica @la_musica @zeta93fm https://t.co/ZDZM4Qs1zx https://t.co/zqMjKMgN4r\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.32 ms /    16 runs   (    0.58 ms per token,  1717.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.88 ms /    89 tokens (    3.28 ms per token,   304.92 tokens per second)\n",
            "llama_print_timings:        eval time =    1557.88 ms /    15 runs   (  103.86 ms per token,     9.63 tokens per second)\n",
            "llama_print_timings:       total time =    2139.63 ms /   104 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@Fistroman1 @ElioGatsby Without proof?? What was it in public? What did we all see, what are you telling me? Don't mix churras with merinas. What from now until you come to me with the metoo and the notallmen there's little left and I don't feel like it end up really arguing, I see it coming.\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      10.11 ms /    16 runs   (    0.63 ms per token,  1582.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =     281.86 ms /    83 tokens (    3.40 ms per token,   294.48 tokens per second)\n",
            "llama_print_timings:        eval time =    1528.62 ms /    15 runs   (  101.91 ms per token,     9.81 tokens per second)\n",
            "llama_print_timings:       total time =    2085.57 ms /    98 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@EstefaniaVeloz ...With the legal process, not to set a person on fire (I don't even know who they are in this case). There are women who are victims but there are also corrupt, violent and victimizers. The #MeToo movement has already fulfilled its mission. What's next? How to advance without it being a weapon of whim?\n",
            "[/INST]\n",
            "NO\n"
          ]
        }
      ],
      "source": [
        "replies_list = ['YES','NO']\n",
        "counter = 0\n",
        "\n",
        "# List to store the results.\n",
        "json_result_run1 = []\n",
        "\n",
        "\n",
        "with open('answer.txt', 'w') as f:\n",
        "        f.write('')\n",
        "\n",
        "# simple JSON loading\n",
        "with open(test_path, 'r') as istr:\n",
        "    test_json = json.load(istr)\n",
        "num_sample = len(test_json)\n",
        "#print(num_sample)\n",
        "\n",
        "#print(train_1_json[0])\n",
        "for key, value in test_json.items():\n",
        "  counter+=1\n",
        "  tweet = test_json[key]['tweet']\n",
        "  if test_json[key]['lang']=='es':\n",
        "      tweet = GoogleTranslator(source='es', target='en').translate(test_json[key]['tweet'])\n",
        "\n",
        "  current_sample = '[INST]\\n'+tweet+'\\n[/INST]'\n",
        "  prompt = few_shot_samples_run1+current_sample\n",
        "\n",
        "  #print(prompt)\n",
        "\n",
        "  response = lcpp_llm(\n",
        "        prompt=prompt,\n",
        "        temperature= 0.2,\n",
        "        logprobs=1,\n",
        "        #max_tokens =1\n",
        "      )\n",
        "\n",
        "  #print(response)\n",
        "\n",
        "  answer = str(response[\"choices\"][0][\"text\"]).strip().upper()\n",
        "  #print(answer)\n",
        "  answer = answer[:3]\n",
        "  #answer = answer.split()[0]\n",
        "  # Sometime output contains a '.' remove it!\n",
        "  #answer = answer.replace('.','')\n",
        "\n",
        "  # If the predicted word is not in emotion list just replace with neutral.\n",
        "  if answer not in replies_list:\n",
        "        answer = 'NO'\n",
        "\n",
        "  #current_sample += answer + \" \\n \"\n",
        "\n",
        "  print(\"GENERATED: \"+ current_sample+'\\n'+answer)\n",
        "\n",
        "  current_element = {\n",
        "        \"id\": test_json[key]['id'],\n",
        "        \"value\": answer,\n",
        "        \"test_case\": \"EXIST2024\"\n",
        "    }\n",
        "  json_result_run1.append(current_element)\n",
        "\n",
        "with open(\"task1_hard_badrock_1.json\", \"w\") as json_file:\n",
        "    json.dump(json_result_run1, json_file, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "replies_list = ['YES','NO']\n",
        "counter = 0\n",
        "\n",
        "# List to store the results.\n",
        "json_result_run2 = []\n",
        "\n",
        "# simple JSON loading\n",
        "with open(test_path, 'r') as istr:\n",
        "    test_json = json.load(istr)\n",
        "num_sample = len(test_json)\n",
        "#print(num_sample)\n",
        "\n",
        "#print(train_1_json[0])\n",
        "for key, value in test_json.items():\n",
        "  counter+=1\n",
        "  tweet = test_json[key]['tweet']\n",
        "  if test_json[key]['lang']=='es':\n",
        "      tweet = GoogleTranslator(source='es', target='en').translate(test_json[key]['tweet'])\n",
        "\n",
        "  current_sample = '[INST]\\n'+tweet+'\\n[/INST]'\n",
        "  prompt = few_shot_samples_run2+current_sample\n",
        "\n",
        "  #print(prompt)\n",
        "\n",
        "  response = lcpp_llm(\n",
        "        prompt=prompt,\n",
        "        temperature= 0.2,\n",
        "        logprobs=1,\n",
        "        #max_tokens =1\n",
        "      )\n",
        "\n",
        "  #print(response)\n",
        "\n",
        "  answer = str(response[\"choices\"][0][\"text\"]).strip().upper()\n",
        "  #print(answer)\n",
        "  answer = answer[:3]\n",
        "  #answer = answer.split()[0]\n",
        "  # Sometime output contains a '.' remove it!\n",
        "  #answer = answer.replace('.','')\n",
        "\n",
        "  # If the predicted word is not in emotion list just replace with neutral.\n",
        "  if answer not in replies_list:\n",
        "        answer = 'NO'\n",
        "\n",
        "  #current_sample += answer + \" \\n \"\n",
        "\n",
        "  print(\"GENERATED: \"+ current_sample+'\\n'+answer)\n",
        "\n",
        "  current_element = {\n",
        "        \"id\": test_json[key]['id'],\n",
        "        \"value\": answer,\n",
        "        \"test_case\": \"EXIST2024\"\n",
        "    }\n",
        "  json_result_run2.append(current_element)\n",
        "\n",
        "with open(\"task1_hard_badrock_2.json\", \"w\") as json_file:\n",
        "    json.dump(json_result_run2, json_file, indent=4)\n"
      ],
      "metadata": {
        "id": "4SfpJbGEa0R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r exist2024_badrock.zip task1_hard_badrock_1.json task1_hard_badrock_2.json"
      ],
      "metadata": {
        "id": "JAUcpxAdivmY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1649bd0e-3706-4513-ee96-d40b0061e3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: answer.txt (deflated 94%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('exist2024_badrock.zip')"
      ],
      "metadata": {
        "id": "dia3xJw7TwSL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "4c132f11-547a-414d-f7fa-fbe838eb1735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4d855f4e-51a6-4ae4-a1e9-bf6dd792fe0d\", \"results.zip\", 4729)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b681ea880f56420eacb3fc0366dc65b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da972865bee547b6998fcc38e88165dc",
              "IPY_MODEL_5b19be7f8ae645c6a264d794091d6940",
              "IPY_MODEL_2f04869e637542018dee1995a3fca139"
            ],
            "layout": "IPY_MODEL_858936563e4442b0a0bc168a9b30d3dc"
          }
        },
        "da972865bee547b6998fcc38e88165dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7de4cf1cfff847c39b30876b4ef2d1e4",
            "placeholder": "​",
            "style": "IPY_MODEL_800e8c46c9ae4a118168b9e0ed9bd453",
            "value": "mistral-7b-instruct-v0.2.Q6_K.gguf: 100%"
          }
        },
        "5b19be7f8ae645c6a264d794091d6940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea0a151eec96406281bfcfa3a70245bf",
            "max": 5942065440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cec3fa17191843d089bf40785a6de01b",
            "value": 5942065440
          }
        },
        "2f04869e637542018dee1995a3fca139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7dbe5f4d8614b1992e8a1bf66783ef3",
            "placeholder": "​",
            "style": "IPY_MODEL_0e0454888c0647f4a0ecf54b19488707",
            "value": " 5.94G/5.94G [00:35&lt;00:00, 241MB/s]"
          }
        },
        "858936563e4442b0a0bc168a9b30d3dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7de4cf1cfff847c39b30876b4ef2d1e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "800e8c46c9ae4a118168b9e0ed9bd453": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea0a151eec96406281bfcfa3a70245bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cec3fa17191843d089bf40785a6de01b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7dbe5f4d8614b1992e8a1bf66783ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e0454888c0647f4a0ecf54b19488707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}