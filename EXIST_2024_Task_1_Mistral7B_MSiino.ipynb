{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/EXIST2024/blob/main/EXIST_2024_Task_1_Mistral7B_MSiino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ubSCVmmlBKX"
      },
      "source": [
        "Installing dependencies. You might need to tweak the CMAKE_ARGS for the `llama-cpp-python` pip package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKL68Itp9Bm-",
        "outputId": "ff876f14-7993-4d76-b5f8-79836dd2acd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python>=0.1.79\n",
            "  Downloading llama_cpp_python-0.2.71.tar.gz (48.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m48.4/48.4 MB\u001b[0m \u001b[31m173.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m192.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m176.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python>=0.1.79)\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m210.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python>=0.1.79)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.71-cp310-cp310-linux_x86_64.whl size=55332750 sha256=d976022a19ae95f0de4f306688a912d23a87fc5ea7a1eb5d876f6c3f25880789\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7oc3mfnv/wheels/5d/35/d3/b4ab42b4528cc5e1f9b1469c4b4076de51e2f3ceb50506b424\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.3\n",
            "    Uninstalling Jinja2-3.1.3:\n",
            "      Successfully uninstalled Jinja2-3.1.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.71 numpy-1.26.4 typing-extensions-4.11.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.2.2)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ],
      "source": [
        "# GPU llama-cpp-python; Starting from version llama-cpp-python==0.1.79, it supports GGUF\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on \" pip install 'llama-cpp-python>=0.1.79' --force-reinstall --upgrade --no-cache-dir\n",
        "# For download the models\n",
        "!pip install huggingface_hub\n",
        "!pip install datasets\n",
        "!pip install -U deep-translator\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from deep_translator import GoogleTranslator\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import tqdm.notebook as tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2ns57iDlBKa"
      },
      "source": [
        "Downloading an instruction-finetuned Mistral model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uDMqQmBfAhYO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2f9ce44597fd4605ae80a651595427b8",
            "e3b28ef3d43f485eab6249e38bce939d",
            "422514713b674c6fa08e3ccfec13d587",
            "f1af885bd06a414194141b2b4caba3c5",
            "bcd7b0cb09fd4011817744131ed5786b",
            "ecc8fceff5be4c74a8e8078c30a0537e",
            "066a845b3b494ffc86eaf3b42f4f1be8",
            "8a17530926d34ebc81cf106d8a1a1892",
            "ee360156296d477ead496a7516f8c40b",
            "00742ad8442849b7884fc2771237645e",
            "4fd3deff3d1f40b693b7c2722c1cc959"
          ]
        },
        "outputId": "d7438c44-9af1-48fc-eebc-4a417f642628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mistral-7b-instruct-v0.2.Q6_K.gguf:   0%|          | 0.00/5.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f9ce44597fd4605ae80a651595427b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q6_K.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 18\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q6_K:  226 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q6_K\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 5.53 GiB (6.56 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 32/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  5666.09 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  5461.00 MiB\n",
            "...................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 8192\n",
            "llama_new_context_with_model: n_batch    = 800\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 4\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n",
            "Guessed chat format: mistral-instruct\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\"\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "# This config has been tested on an RTX 3080 (VRAM of 16GB).\n",
        "# you might need to tweak with respect to your hardware.\n",
        "from llama_cpp import Llama\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=4, #16, # CPU cores\n",
        "    n_batch=800, #8000, # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=32, # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=8192, # Context window\n",
        "    logits_all=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset for the three subtasks."
      ],
      "metadata": {
        "id": "jeXgLOpd4ztp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subtask 1.\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/EXIST2024/main/dataset/EXIST2023_test_clean.json'\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/EXIST2024/main/dataset/EXIST2024_training.json'\n",
        "!wget 'https://raw.githubusercontent.com/marco-siino/EXIST2024/main/dataset/EXIST2024_training_task1_gold_hard.json'\n",
        "\n",
        "\n",
        "train_path = 'EXIST2024_training.json'\n",
        "train_gold_path = 'EXIST2024_training_task1_gold_hard.json'\n",
        "test_path = 'EXIST2023_test_clean.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKaC_tQ-2xbI",
        "outputId": "72e770b9-9b47-4b98-b00c-0a965009ea83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-09 10:33:51--  https://raw.githubusercontent.com/marco-siino/EXIST2024/main/dataset/EXIST2023_test_clean.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2143912 (2.0M) [text/plain]\n",
            "Saving to: ‚ÄòEXIST2023_test_clean.json‚Äô\n",
            "\n",
            "EXIST2023_test_clea 100%[===================>]   2.04M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-05-09 10:33:51 (87.2 MB/s) - ‚ÄòEXIST2023_test_clean.json‚Äô saved [2143912/2143912]\n",
            "\n",
            "--2024-05-09 10:33:51--  https://raw.githubusercontent.com/marco-siino/EXIST2024/main/dataset/EXIST2024_training.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9397366 (9.0M) [text/plain]\n",
            "Saving to: ‚ÄòEXIST2024_training.json‚Äô\n",
            "\n",
            "EXIST2024_training. 100%[===================>]   8.96M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-05-09 10:33:52 (210 MB/s) - ‚ÄòEXIST2024_training.json‚Äô saved [9397366/9397366]\n",
            "\n",
            "--2024-05-09 10:33:52--  https://raw.githubusercontent.com/marco-siino/EXIST2024/main/dataset/EXIST2024_training_task1_gold_hard.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 433241 (423K) [text/plain]\n",
            "Saving to: ‚ÄòEXIST2024_training_task1_gold_hard.json‚Äô\n",
            "\n",
            "EXIST2024_training_ 100%[===================>] 423.09K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-05-09 10:33:52 (32.7 MB/s) - ‚ÄòEXIST2024_training_task1_gold_hard.json‚Äô saved [433241/433241]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create few-shot samples from training and validation set."
      ],
      "metadata": {
        "id": "fHPX3E0PCJ9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " prompt_context = \"<s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\\n\"\n"
      ],
      "metadata": {
        "id": "zafRW5vU6y1n"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_few_shot_samples_task1(json_set_path,nr_samples):\n",
        "\n",
        "  nr_few_shot_samples = nr_samples\n",
        "\n",
        "  few_shot_counter = 0\n",
        "\n",
        "  few_shot_samples = ''\n",
        "\n",
        "  with open(train_gold_path, 'r') as istr:\n",
        "    train_gold_json = json.load(istr)\n",
        "  indexed_data = {entry[\"id\"]: entry for entry in train_gold_json}\n",
        "  #print(indexed_data['100001'])\n",
        "  with open(json_set_path, 'r') as istr:\n",
        "    train_1_json = json.load(istr)\n",
        "  #print(train_1_json[0])\n",
        "  for key, value in train_1_json.items():\n",
        "  #for i in tqdm.trange(len(train_1_json)):\n",
        "    if few_shot_counter == nr_few_shot_samples:\n",
        "      break\n",
        "\n",
        "\n",
        "    try:\n",
        "      label = indexed_data[key]['value']\n",
        "    except:\n",
        "      continue\n",
        "    #for sentence_nr in range(0,len(train_1_json[i]['speakers'])):\n",
        "    few_shot_counter+=1\n",
        "    tweet = train_1_json[key]['tweet']\n",
        "    print(train_1_json[key]['tweet'] + ' // ' + label)\n",
        "    if train_1_json[key]['lang']=='es':\n",
        "      tweet = GoogleTranslator(source='es', target='en').translate(train_1_json[key]['tweet'])\n",
        "\n",
        "    few_shot_samples += prompt_context+\"TWEET:\"+tweet + ' [/INST] \\n ' + label + \"\\n</s> \\n\\n \"\n",
        "\n",
        "    if few_shot_counter == nr_few_shot_samples:\n",
        "        break\n",
        "\n",
        "  return few_shot_samples"
      ],
      "metadata": {
        "id": "99ag8gav51mC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_samples = create_few_shot_samples_task1(train_path,10)\n",
        "#few_shot_samples += create_few_shot_samples_task1(val_path_1,80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8IaDH-7GmrN",
        "outputId": "f23eb768-e9bc-4901-ce9b-d43dcaa76527"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@TheChiflis Ignora al otro, es un capullo.El problema con este youtuber denuncia el acoso... cuando no afecta a la gente de izquierdas. Por ejemplo, en su video sobre el gamergate presenta como \"normal\" el acoso que reciben Fisher, Anita o Z√∂ey cuando hubo hasta amenazas de bomba. // YES\n",
            "@ultimonomada_ Si comicsgate se parece en algo a gamergate pues muy bien por el acoso. Y si se est√° haciendo un sabotaje porque hay personajes que no os gustan entonces gracias por darme la raz√≥n. Sois unos lloricas ofendidos. // NO\n",
            "@Steven2897 Lee sobre Gamergate, y como eso ha cambiado la manera en la cual nos comunicamos en el internet. Los fanboys de Halo est√°n t√≥xicos pero los fanboys de otras comunidades/juegos tambi√©n han querido coger pauta con eso ü§∑üèæ‚Äç‚ôÇÔ∏è // NO\n",
            "@novadragon21 @icep4ck @TvDannyZ Entonces como as√≠ es el mercado lo mejor no es hacer algo para cambiarlo y seguir alimentando el machismo en los consumidores en lugar apoyar a gente como las v√≠ctimas del gamergate.Acerca de lo otro, el \"ten√≠an\" implica un imperativo entonces no entiendo lo del buscaban. // YES\n",
            "@yonkykong Aaah s√≠. Andrew Dobson. El que se dedic√≥ a echar mierda del gamergate al m√°s puro estilo white knight, y todo deviantart se le ech√≥ encima // NO\n",
            "@BestKabest Esta gringa sigue llorando por el gamergate, que \"coincidencia\" que tenga pronombres en su perfil // YES\n",
            ".¬øConoces la #DECORACION #estilo #GAMER para #adolescentes  ?Te lo explicamos en nuestro #post : ‚ù§Ô∏è https://t.co/NuSNs36GIT¬øNos sigues?#inspiracionalcuadrado #interiorismo #homedecorideas #Madrid #chamberi #Salamanca #gamers #GAMERGATE #gamergirl https://t.co/Mcibiy8YdX // NO\n",
            "CES 2022 ASUS ROG Rise of Gamers Evento de lanzamiento https://t.co/CjqHQwxMFD #AMD2022 #AMD #AMDRedTeam #AMDRyzen #Intel #nvidia #NVIDIAOmniverse #NVIDIAGeForce #NVIDIAStudio #gamergirl #gamers #GamerGate #GamersUnite #gameroom #StockMarket #stocks #StocksToBuy #Stock // NO\n",
            "@Lorebrou16 @arrobaDonko Mierda, me siento vieja cuando dec√≠s Gamergate, que bardo .__.XD // NO\n",
            "@Recuenco @qlopdopis Bannon tiene tambi√©n historia.Aqu√≠ hablan de √©l, buen podcast, #16 - los ganadores de Gamergate https://t.co/nnfOHYhbrw // NO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(few_shot_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WHgxqxAxcu1",
        "outputId": "e0ec2855-fc2b-4d96-c75d-e17167ae66bf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@TheChiflis Ignore the other one, he's an idiot. The problem with this YouTuber is that he denounces harassment... when it doesn't affect people on the left. For example, in his video about gamergate he presents as \"normal\" the harassment that Fisher, Anita or Z√∂ey receive when there were even bomb threats. [/INST] \n",
            " YES\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@ultimonomada_ If comicsgate is anything like gamergate, then good for the harassment. And if sabotage is being done because there are characters that you don't like, then thank you for agreeing with me. You are offended crybabies. [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@Steven2897 Read about Gamergate, and how it has changed the way we communicate on the internet. Halo fanboys are toxic but fanboys from other communities/games have also wanted to take a cue from that ü§∑üèæ‚Äç‚ôÇÔ∏è [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@novadragon21 @icep4ck @TvDannyZ So since this is the market, the best thing is not to do something to change it and continue feeding machismo in consumers instead of supporting people like the victims of gamergate. Regarding the other, the \"had\" implies a imperative then I don't understand what they were looking for. [/INST] \n",
            " YES\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@yonkykong Aaah yes. Andrew Dobson. The one who dedicated himself to spouting gamergate shit in the purest white knight style, and all of deviantart fell on him [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@BestKabest This gringa is still crying about gamergate, what a \"coincidence\" that she has pronouns on her profile [/INST] \n",
            " YES\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:.Do you know the #GAMER #style #DECORATION for #teenagers? We explain it to you in our #post: ‚ù§Ô∏è https://t.co/NuSNs36GIT Are you following us? #inspiracionalcuadrado #interiorismo #homedecorideas #Madrid #chamberi #Salamanca #gamers #GAMERGATE #gamergirl https://t.co/Mcibiy8YdX [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:CES 2022 ASUS ROG Rise of Gamers Launch Event https://t.co/CjqHQwxMFD #AMD2022 #AMD #AMDRedTeam #AMDRyzen #Intel #nvidia #NVIDIAOmniverse #NVIDIAGeForce #NVIDIAStudio #gamergirl #gamers #GamerGate #GamersUnite #gameroom #StockMarket # stocks #StocksToBuy #Stock [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@Lorebrou16 @arrobaDonko Shit, I feel old when you say Gamergate, what a bard .__.XD [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " <s>[INST] Is the following TWEET sexist, in any form, or does it describe situations in which such discrimination occurs (i.e., it is sexist itself, describes a sexist situation or criticizes a sexist behaviour)? Reply only with YES or NO.\n",
            "TWEET:@Recuenco @qlopdopis Bannon also has history. Here they talk about him, good podcast, #16 - the winners of Gamergate https://t.co/nnfOHYhbrw [/INST] \n",
            " NO\n",
            "</s> \n",
            "\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run!"
      ],
      "metadata": {
        "id": "r6Cqo0JqotN9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKo1-X5OvT4b",
        "outputId": "9afc2e95-7d26-4ec6-d8ad-d059faebdd63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      11.26 ms /    16 runs   (    0.70 ms per token,  1421.46 tokens per second)\n",
            "llama_print_timings: prompt eval time =     953.35 ms /    31 tokens (   30.75 ms per token,    32.52 tokens per second)\n",
            "llama_print_timings:        eval time =    1825.91 ms /    15 runs   (  121.73 ms per token,     8.22 tokens per second)\n",
            "llama_print_timings:       total time =    3000.86 ms /    46 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@Eurogamer_es All gamergate from development to game forums, classic in the world of video games.\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       8.84 ms /    16 runs   (    0.55 ms per token,  1809.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =     554.04 ms /    32 tokens (   17.31 ms per token,    57.76 tokens per second)\n",
            "llama_print_timings:        eval time =    1479.82 ms /    15 runs   (   98.65 ms per token,    10.14 tokens per second)\n",
            "llama_print_timings:       total time =    2169.46 ms /    47 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@ArCaNgEl__23 @Benzenazi Man, it's not comparable, look at Gamergate.\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.09 ms /    16 runs   (    0.57 ms per token,  1760.18 tokens per second)\n",
            "llama_print_timings: prompt eval time =     577.26 ms /    24 tokens (   24.05 ms per token,    41.58 tokens per second)\n",
            "llama_print_timings:        eval time =    1508.51 ms /    15 runs   (  100.57 ms per token,     9.94 tokens per second)\n",
            "llama_print_timings:       total time =    2204.03 ms /    39 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "I'm looking for companies involved in gamergate for etc etc to buy all their products\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      10.42 ms /    16 runs   (    0.65 ms per token,  1535.95 tokens per second)\n",
            "llama_print_timings: prompt eval time =     266.35 ms /    75 tokens (    3.55 ms per token,   281.59 tokens per second)\n",
            "llama_print_timings:        eval time =    1706.46 ms /    15 runs   (  113.76 ms per token,     8.79 tokens per second)\n",
            "llama_print_timings:       total time =    2234.81 ms /    90 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@jordirico First it was the internet, then the gamergate, the manosphere and its extreme misogyny without the platforms lifting a finger, the incel-murders, the extreme right capitalizing on hate, the mimicry of these ideas with those of a respectable debate and finally here we are\n",
            "[/INST]\n",
            "YES\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      11.24 ms /    16 runs   (    0.70 ms per token,  1423.11 tokens per second)\n",
            "llama_print_timings: prompt eval time =     859.61 ms /    28 tokens (   30.70 ms per token,    32.57 tokens per second)\n",
            "llama_print_timings:        eval time =    1912.53 ms /    15 runs   (  127.50 ms per token,     7.84 tokens per second)\n",
            "llama_print_timings:       total time =    2988.06 ms /    43 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@AlonsoQuijano12 I was involved in gamergate, so unfortunately I know too much üò≠\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       8.94 ms /    16 runs   (    0.56 ms per token,  1788.91 tokens per second)\n",
            "llama_print_timings: prompt eval time =     775.03 ms /    29 tokens (   26.73 ms per token,    37.42 tokens per second)\n",
            "llama_print_timings:        eval time =    1532.26 ms /    15 runs   (  102.15 ms per token,     9.79 tokens per second)\n",
            "llama_print_timings:       total time =    2442.70 ms /    44 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@MrSandman1954 @AKN4710 Gamergate screwed the internet forever.\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.32 ms /    16 runs   (    0.58 ms per token,  1716.37 tokens per second)\n",
            "llama_print_timings: prompt eval time =     299.13 ms /   102 tokens (    2.93 ms per token,   340.99 tokens per second)\n",
            "llama_print_timings:        eval time =    1500.46 ms /    15 runs   (  100.03 ms per token,    10.00 tokens per second)\n",
            "llama_print_timings:       total time =    2122.31 ms /   117 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@grupoeldeber @grupoeldeber It would be good if they also said that it was in CANADAüá®üá¶ the first country where it was reported that G‚àÜy couples who adopted children abused them in 2008. How selective are they with the crap they publish, right? https://t.co/GPAmLOfcY9 https://t.co/f36aAmxq35\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.13 ms /    16 runs   (    0.57 ms per token,  1752.85 tokens per second)\n",
            "llama_print_timings: prompt eval time =     252.47 ms /    40 tokens (    6.31 ms per token,   158.43 tokens per second)\n",
            "llama_print_timings:        eval time =    1557.09 ms /    15 runs   (  103.81 ms per token,     9.63 tokens per second)\n",
            "llama_print_timings:       total time =    1971.28 ms /    55 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "They, very close together and more alone than ever. The consequences of everything. Mozambique. https://t.co/5nVGTb3Ua3\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       8.99 ms /    16 runs   (    0.56 ms per token,  1779.16 tokens per second)\n",
            "llama_print_timings: prompt eval time =     586.76 ms /    32 tokens (   18.34 ms per token,    54.54 tokens per second)\n",
            "llama_print_timings:        eval time =    1530.86 ms /    15 runs   (  102.06 ms per token,     9.80 tokens per second)\n",
            "llama_print_timings:       total time =    2260.74 ms /    47 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "DC Children's Hospital Harassed Over Trans Youth Serviceshttps://t.co/RrEtZ5TDDM\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.33 ms /    16 runs   (    0.58 ms per token,  1714.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     256.55 ms /    45 tokens (    5.70 ms per token,   175.40 tokens per second)\n",
            "llama_print_timings:        eval time =    1568.85 ms /    15 runs   (  104.59 ms per token,     9.56 tokens per second)\n",
            "llama_print_timings:       total time =    2011.60 ms /    60 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "20mins deep into BeReal and I have never felt so pressured to take my phone nope I don't want to feel harassed I say goodbye ü´°\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      11.79 ms /    16 runs   (    0.74 ms per token,  1357.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     271.85 ms /    61 tokens (    4.46 ms per token,   224.39 tokens per second)\n",
            "llama_print_timings:        eval time =    1789.32 ms /    15 runs   (  119.29 ms per token,     8.38 tokens per second)\n",
            "llama_print_timings:       total time =    2435.20 ms /    76 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "Anthony Fauci, the leading US infectious disease expert, noted in August 2022 that he had received death threats and that his family was routinely exposed to harassment. https://t.co/EerbO2Px5j\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       8.92 ms /    16 runs   (    0.56 ms per token,  1794.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =     699.63 ms /    28 tokens (   24.99 ms per token,    40.02 tokens per second)\n",
            "llama_print_timings:        eval time =    1435.95 ms /    15 runs   (   95.73 ms per token,    10.45 tokens per second)\n",
            "llama_print_timings:       total time =    2263.21 ms /    43 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@dimplerrylover I know but literally the girl was being harassed like how can she smile like that\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.62 ms /    16 runs   (    0.60 ms per token,  1663.20 tokens per second)\n",
            "llama_print_timings: prompt eval time =     284.15 ms /    85 tokens (    3.34 ms per token,   299.14 tokens per second)\n",
            "llama_print_timings:        eval time =    1480.67 ms /    15 runs   (   98.71 ms per token,    10.13 tokens per second)\n",
            "llama_print_timings:       total time =    2055.73 ms /   100 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@marcela_ruge @_Lamanada_ @Mitocondria1311 @karinin7986 @julibmoya @MissPoirot07 @Adrianaruiz17 @DiaAnimales @marcevalenciar @MONYRODRIGUEZOF Marce do you have daviplata?\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      11.71 ms /    16 runs   (    0.73 ms per token,  1366.47 tokens per second)\n",
            "llama_print_timings: prompt eval time =     260.71 ms /    64 tokens (    4.07 ms per token,   245.48 tokens per second)\n",
            "llama_print_timings:        eval time =    1804.13 ms /    15 runs   (  120.28 ms per token,     8.31 tokens per second)\n",
            "llama_print_timings:       total time =    2311.64 ms /    79 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "About to see the grand premiere of #Jaur√≠a based on the case of #LaManada @anasofihg @RobertoBeck90 @elteatrocdmx #teatromexicano https://t.co/N5Uke18ikQ\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.59 ms /    16 runs   (    0.60 ms per token,  1668.93 tokens per second)\n",
            "llama_print_timings: prompt eval time =     259.75 ms /    64 tokens (    4.06 ms per token,   246.39 tokens per second)\n",
            "llama_print_timings:        eval time =    1512.29 ms /    15 runs   (  100.82 ms per token,     9.92 tokens per second)\n",
            "llama_print_timings:       total time =    1995.70 ms /    79 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "What a bastard our Uncle Pablo is with his little jokes. How comfortable we are on the cool grass... mybestfriends #doglove #theyneverabandon #lodaneverythingchangedenada https://t.co/4x20oe7V8D\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      10.20 ms /    16 runs   (    0.64 ms per token,  1569.24 tokens per second)\n",
            "llama_print_timings: prompt eval time =     289.27 ms /    75 tokens (    3.86 ms per token,   259.28 tokens per second)\n",
            "llama_print_timings:        eval time =    1606.43 ms /    15 runs   (  107.10 ms per token,     9.34 tokens per second)\n",
            "llama_print_timings:       total time =    2152.33 ms /    90 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@noebermudez_13 @ldacr And we are already looking forward to filling the stands with you everywhere again!!! @LaManada_CR And now distribute balls left and right except to the ball gatherers ü•∫ü§£ü§£ü§£ü§£#VAMOSLEONAS #SOMOSLAMANADA\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      10.94 ms /    16 runs   (    0.68 ms per token,  1462.12 tokens per second)\n",
            "llama_print_timings: prompt eval time =     311.53 ms /   116 tokens (    2.69 ms per token,   372.36 tokens per second)\n",
            "llama_print_timings:        eval time =    1832.49 ms /    15 runs   (  122.17 ms per token,     8.19 tokens per second)\n",
            "llama_print_timings:       total time =    2545.53 ms /   131 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "#TBT: Mar√≠a Rosa, Gregorio Polanco and Ysabel Ver√°s, back in October 2016, during one of the Kerm√©s held for the team's fundraising. The Pack is tradition, family, unity and joy!ü•éüê∫ü•éüê∫#TBT #Auuuuu#LaManada#LobosFansClub#LobosSoftbolClub https://t.co/LwSL4npanZ\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.20 ms /    16 runs   (    0.58 ms per token,  1738.56 tokens per second)\n",
            "llama_print_timings: prompt eval time =     382.56 ms /   129 tokens (    2.97 ms per token,   337.21 tokens per second)\n",
            "llama_print_timings:        eval time =    1610.81 ms /    15 runs   (  107.39 ms per token,     9.31 tokens per second)\n",
            "llama_print_timings:       total time =    2401.78 ms /   144 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "AUGUST 9 Find out everything! üéô You can watch the entire segment of 'El Bla, Bla, Bla' with H√âCTOR JOAQU√çN in the #Podcasts section of the LaMusica app. üì≤#HectorJoaquin #LaManada #ElBlaBlaBla #LaMusica @la_musica @zeta93fm https://t.co/ZDZM4Qs1zx https://t.co/zqMjKMgN4r\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =       9.32 ms /    16 runs   (    0.58 ms per token,  1717.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =     291.88 ms /    89 tokens (    3.28 ms per token,   304.92 tokens per second)\n",
            "llama_print_timings:        eval time =    1557.88 ms /    15 runs   (  103.86 ms per token,     9.63 tokens per second)\n",
            "llama_print_timings:       total time =    2139.63 ms /   104 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@Fistroman1 @ElioGatsby Without proof?? What was it in public? What did we all see, what are you telling me? Don't mix churras with merinas. What from now until you come to me with the metoo and the notallmen there's little left and I don't feel like it end up really arguing, I see it coming.\n",
            "[/INST]\n",
            "NO\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =    1255.65 ms\n",
            "llama_print_timings:      sample time =      10.11 ms /    16 runs   (    0.63 ms per token,  1582.90 tokens per second)\n",
            "llama_print_timings: prompt eval time =     281.86 ms /    83 tokens (    3.40 ms per token,   294.48 tokens per second)\n",
            "llama_print_timings:        eval time =    1528.62 ms /    15 runs   (  101.91 ms per token,     9.81 tokens per second)\n",
            "llama_print_timings:       total time =    2085.57 ms /    98 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERATED: [INST]\n",
            "@EstefaniaVeloz ...With the legal process, not to set a person on fire (I don't even know who they are in this case). There are women who are victims but there are also corrupt, violent and victimizers. The #MeToo movement has already fulfilled its mission. What's next? How to advance without it being a weapon of whim?\n",
            "[/INST]\n",
            "NO\n"
          ]
        }
      ],
      "source": [
        "replies_list = ['YES','NO']\n",
        "counter = 0\n",
        "with open('answer.txt', 'w') as f:\n",
        "        f.write('')\n",
        "\n",
        "# simple JSON loading\n",
        "with open(test_path, 'r') as istr:\n",
        "    test_json = json.load(istr)\n",
        "num_sample = len(test_json)\n",
        "#print(num_sample)\n",
        "\n",
        "#print(train_1_json[0])\n",
        "for key, value in test_json.items():\n",
        "  counter+=1\n",
        "  tweet = test_json[key]['tweet']\n",
        "  if test_json[key]['lang']=='es':\n",
        "      tweet = GoogleTranslator(source='es', target='en').translate(test_json[key]['tweet'])\n",
        "\n",
        "  current_sample = '[INST]\\n'+tweet+'\\n[/INST]'\n",
        "  prompt = few_shot_samples+current_sample\n",
        "\n",
        "  #print(prompt)\n",
        "\n",
        "  response = lcpp_llm(\n",
        "        prompt=prompt,\n",
        "        temperature= 0.2,\n",
        "        logprobs=1,\n",
        "        #max_tokens =1\n",
        "      )\n",
        "\n",
        "  #print(response)\n",
        "\n",
        "  answer = str(response[\"choices\"][0][\"text\"]).strip().upper()\n",
        "  #print(answer)\n",
        "  answer = answer[:3]\n",
        "  #answer = answer.split()[0]\n",
        "  # Sometime output contains a '.' remove it!\n",
        "  #answer = answer.replace('.','')\n",
        "\n",
        "  # If the predicted word is not in emotion list just replace with neutral.\n",
        "  if answer not in replies_list:\n",
        "        answer = 'NO'\n",
        "\n",
        "  #current_sample += answer + \" \\n \"\n",
        "\n",
        "  print(\"GENERATED: \"+ current_sample+'\\n'+answer)\n",
        "\n",
        "\n",
        "  with open('answer.txt', 'a') as f:\n",
        "    f.write(answer+\"\\n\")\n",
        "  if counter == 20:\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counter=0\n",
        "for i in range(0,len(test_1_json)):\n",
        "  for sentence_nr in range(0,len(test_1_json[i]['utterances'])):\n",
        "    counter+=1\n",
        "\n",
        "print(counter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "209aetruI4p0",
        "outputId": "7d8d6fa5-2040-4bde-dcce-bb74fa1f05bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "open('newfile','w').writelines([ line for line in open('SemEval_2024_Task_10_Mistral7B_MSiino (1).ipynb') if 'GENERATED:' in line])\n",
        "\n",
        "result_not_filtered = [ line for line in open('SemEval_2024_Task_10_Mistral7B_MSiino (1).ipynb') if 'GENERATED:' in line]\n"
      ],
      "metadata": {
        "id": "VR-Z_ME9O6W0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('answer.txt', 'w') as f:\n",
        "        f.write('')"
      ],
      "metadata": {
        "id": "XkFIe4mNUPUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('newfile') as f:\n",
        "   for line in f:\n",
        "       # For Python3, use print(line)\n",
        "       x = line.split()\n",
        "       result = x[len(x)-1].replace('\\\\n\",','')\n",
        "       result = result.replace('\\\\n\"','')\n",
        "       print (result)\n",
        "       with open('answer.txt', 'a') as f:\n",
        "        f.write(result+\"\\n\")"
      ],
      "metadata": {
        "id": "SzlBus5ZSM0C",
        "outputId": "9cc0c303-157e-4ed5-a85c-30fa5c95eecd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sadness\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "anger\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "fear\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "contempt\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "disgust\n",
            "fear\n",
            "surprise\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "anger\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "surprise\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "anger\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "anger\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "anger\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "surprise\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "fear\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "fear\n",
            "neutral\n",
            "anger\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "disgust\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "contempt\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "anger\n",
            "joy\n",
            "neutral\n",
            "fear\n",
            "joy\n",
            "joy\n",
            "disgust\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "sadness\n",
            "joy\n",
            "disgust\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "anger\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "contempt\n",
            "surprise\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "fear\n",
            "joy\n",
            "contempt\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "fear\n",
            "sadness\n",
            "fear\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "sadness\n",
            "joy\n",
            "sadness\n",
            "joy\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "joy\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "surprise\n",
            "neutral\n",
            "anger\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "joy\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "surprise\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "surprise\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "sadness\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "contempt\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "contempt\n",
            "neutral\n",
            "contempt\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "contempt\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "surprise\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "surprise\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "anger\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "joy\n",
            "contempt\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "surprise\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "contempt\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "joy\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "surprise\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "contempt\n",
            "contempt\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "surprise\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "fear\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "surprise\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "surprise\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "disgust\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "surprise\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "surprise\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "joy\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "disgust\n",
            "sadness\n",
            "joy\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "contempt\n",
            "neutral\n",
            "surprise\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "joy\n",
            "surprise\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "disgust\n",
            "neutral\n",
            "disgust\n",
            "contempt\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "contempt\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "disgust\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "fear\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "fear\n",
            "fear\n",
            "fear\n",
            "fear\n",
            "fear\n",
            "neutral\n",
            "anger\n",
            "contempt\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "anger\n",
            "anger\n",
            "neutral\n",
            "joy\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "anger\n",
            "anger\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "anger\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "anger\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "joy\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "fear\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "sadness\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "disgust\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "neutral\n",
            "surprise\n",
            "joy\n",
            "neutral\n",
            "current_sample+answer)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now fill the remaining lines for the other two tasks.\n",
        "\n",
        "for i in range(1581,17913):\n",
        "  r_int_value = random.randint(0, 1)\n",
        "  r_string_value = str(r_int_value)+'.0'\n",
        "  with open('answer.txt', 'a') as f:\n",
        "        f.write(r_string_value+\"\\n\")\n"
      ],
      "metadata": {
        "id": "vFIWPRbu94e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r results.zip answer.txt"
      ],
      "metadata": {
        "id": "JAUcpxAdivmY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1649bd0e-3706-4513-ee96-d40b0061e3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: answer.txt (deflated 94%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('results.zip')"
      ],
      "metadata": {
        "id": "dia3xJw7TwSL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "4c132f11-547a-414d-f7fa-fbe838eb1735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4d855f4e-51a6-4ae4-a1e9-bf6dd792fe0d\", \"results.zip\", 4729)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f9ce44597fd4605ae80a651595427b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e3b28ef3d43f485eab6249e38bce939d",
              "IPY_MODEL_422514713b674c6fa08e3ccfec13d587",
              "IPY_MODEL_f1af885bd06a414194141b2b4caba3c5"
            ],
            "layout": "IPY_MODEL_bcd7b0cb09fd4011817744131ed5786b"
          }
        },
        "e3b28ef3d43f485eab6249e38bce939d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecc8fceff5be4c74a8e8078c30a0537e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_066a845b3b494ffc86eaf3b42f4f1be8",
            "value": "mistral-7b-instruct-v0.2.Q6_K.gguf:‚Äá100%"
          }
        },
        "422514713b674c6fa08e3ccfec13d587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a17530926d34ebc81cf106d8a1a1892",
            "max": 5942065440,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee360156296d477ead496a7516f8c40b",
            "value": 5942065440
          }
        },
        "f1af885bd06a414194141b2b4caba3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00742ad8442849b7884fc2771237645e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4fd3deff3d1f40b693b7c2722c1cc959",
            "value": "‚Äá5.94G/5.94G‚Äá[00:44&lt;00:00,‚Äá121MB/s]"
          }
        },
        "bcd7b0cb09fd4011817744131ed5786b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecc8fceff5be4c74a8e8078c30a0537e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "066a845b3b494ffc86eaf3b42f4f1be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a17530926d34ebc81cf106d8a1a1892": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee360156296d477ead496a7516f8c40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00742ad8442849b7884fc2771237645e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fd3deff3d1f40b693b7c2722c1cc959": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}